{"question1_id":1,"question2_id":2,"question3_id":3,"question3_time":1493411946212,"question1_time":1493412756777,"question2_time":1493411668261,"question1_selections":[null,"5","10","10","4","2","3","4","2","5","2"],"question2_selections":[5,7,6,9,3,2,1,8,4,10],"question3_selections":[6,3,2,10,9],"question1_type":3,"question2_type":1,"question3_type":2,"time_start":1493411283486,"time_end":1493413247559,"qualitative_feedback":"Several theorems in game theory and social science [Arrow's, Gibbardâ€“Satterthwaite] state that there's no excellent method of taking preferences from the members of a group and building a set of preferences (or single top choice) that the group would agree with. There might be a loophole around \"separate into a top half and a bottom half\", but it seems unlikely. Whatever mechanism you decide on will be vulnerable to some particular set of participant votes. That said, it may be possible to find a mechanism that works well for common voting patterns.\n\nIt's also not clear why collecting LESS data (ordinal or top-5) would ever be more useful than collecting the full scores, unless perhaps the task of scoring 1-10 is more noisy for reasons of cognitive load and greater breaking of IIA. But certainly whatever math is run on the top-5 data could instead be run on the top 5 of the ranking data. Unless the aim of this research is to show that less taxing question formats are less noisy, I don't see the point.\n\nOverall, I suspect that I'm about to click through and get told I've been lied to and the real purpose of this research is something else altogether. "}